{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RX 9070 XT DirectML Testing\n",
    "\n",
    "Moving from my README.md notes to this notebook for faster iteration. I can get PyTorch to work on DirectML but it feels hacky and I need to work out the details of what exact packages I need etc... I would like to get the requirements locked down so I can test (even if very slow compared to native/ROCm) on this new GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pip list - Current semi-working state\n",
    "\n",
    "```\n",
    "Package                   Version\n",
    "------------------------- ---------------\n",
    "aiofiles                  23.2.1\n",
    "altair                    5.5.0\n",
    "annotated-types           0.7.0\n",
    "anyio                     4.9.0\n",
    "argon2-cffi               23.1.0\n",
    "argon2-cffi-bindings      21.2.0\n",
    "arrow                     1.3.0\n",
    "asttokens                 3.0.0\n",
    "async-lru                 2.0.5\n",
    "attrs                     25.3.0\n",
    "babel                     2.17.0\n",
    "beautifulsoup4            4.13.3\n",
    "bleach                    6.2.0\n",
    "certifi                   2025.1.31\n",
    "cffi                      1.17.1\n",
    "charset-normalizer        3.4.1\n",
    "click                     8.1.8\n",
    "colorama                  0.4.6\n",
    "comm                      0.2.2\n",
    "contourpy                 1.3.1\n",
    "cycler                    0.12.1\n",
    "debugpy                   1.8.13\n",
    "decorator                 5.2.1\n",
    "defusedxml                0.7.1\n",
    "diffusers                 0.32.2\n",
    "executing                 2.2.0\n",
    "fastapi                   0.115.11\n",
    "fastjsonschema            2.21.1\n",
    "ffmpy                     0.5.0\n",
    "filelock                  3.18.0\n",
    "fonttools                 4.56.0\n",
    "fqdn                      1.5.1\n",
    "fsspec                    2025.3.0\n",
    "gradio                    3.48.0\n",
    "gradio_client             0.6.1\n",
    "groovy                    0.1.2\n",
    "h11                       0.14.0\n",
    "httpcore                  1.0.7\n",
    "httpx                     0.28.1\n",
    "huggingface-hub           0.29.3\n",
    "idna                      3.10\n",
    "importlib_metadata        8.6.1\n",
    "importlib_resources       6.5.2\n",
    "ipykernel                 6.29.5\n",
    "ipython                   9.0.2\n",
    "ipython_pygments_lexers   1.1.1\n",
    "isoduration               20.11.0\n",
    "jedi                      0.19.2\n",
    "Jinja2                    3.1.6\n",
    "json5                     0.10.0\n",
    "jsonpointer               3.0.0\n",
    "jsonschema                4.23.0\n",
    "jsonschema-specifications 2024.10.1\n",
    "jupyter_client            8.6.3\n",
    "jupyter_core              5.7.2\n",
    "jupyter-events            0.12.0\n",
    "jupyter-lsp               2.2.5\n",
    "jupyter_server            2.15.0\n",
    "jupyter_server_terminals  0.5.3\n",
    "jupyterlab                4.3.6\n",
    "jupyterlab_pygments       0.3.0\n",
    "jupyterlab_server         2.27.3\n",
    "kiwisolver                1.4.8\n",
    "markdown-it-py            3.0.0\n",
    "MarkupSafe                2.1.5\n",
    "matplotlib                3.10.1\n",
    "matplotlib-inline         0.1.7\n",
    "mdurl                     0.1.2\n",
    "mistune                   3.1.3\n",
    "mpmath                    1.3.0\n",
    "narwhals                  1.31.0\n",
    "nbclient                  0.10.2\n",
    "nbconvert                 7.16.6\n",
    "nbformat                  5.10.4\n",
    "nest-asyncio              1.6.0\n",
    "networkx                  3.4.2\n",
    "notebook_shim             0.2.4\n",
    "numpy                     1.26.4\n",
    "orjson                    3.10.15\n",
    "overrides                 7.7.0\n",
    "packaging                 24.2\n",
    "pandas                    2.2.3\n",
    "pandocfilters             1.5.1\n",
    "parso                     0.8.4\n",
    "pillow                    10.4.0\n",
    "pip                       25.0\n",
    "platformdirs              4.3.6\n",
    "prometheus_client         0.21.1\n",
    "prompt_toolkit            3.0.50\n",
    "psutil                    7.0.0\n",
    "pure_eval                 0.2.3\n",
    "pycparser                 2.22\n",
    "pydantic                  2.10.6\n",
    "pydantic_core             2.27.2\n",
    "pydub                     0.25.1\n",
    "Pygments                  2.19.1\n",
    "pyparsing                 3.2.1\n",
    "python-dateutil           2.9.0.post0\n",
    "python-json-logger        3.3.0\n",
    "python-multipart          0.0.20\n",
    "pytz                      2025.1\n",
    "pywin32                   310\n",
    "pywinpty                  2.0.15\n",
    "PyYAML                    6.0.2\n",
    "pyzmq                     26.3.0\n",
    "referencing               0.36.2\n",
    "regex                     2024.11.6\n",
    "requests                  2.32.3\n",
    "rfc3339-validator         0.1.4\n",
    "rfc3986-validator         0.1.1\n",
    "rich                      13.9.4\n",
    "rpds-py                   0.23.1\n",
    "ruff                      0.11.0\n",
    "safehttpx                 0.1.6\n",
    "safetensors               0.5.3\n",
    "scipy                     1.15.2\n",
    "semantic-version          2.10.0\n",
    "Send2Trash                1.8.3\n",
    "setuptools                75.8.0\n",
    "shellingham               1.5.4\n",
    "six                       1.17.0\n",
    "sniffio                   1.3.1\n",
    "soupsieve                 2.6\n",
    "stack-data                0.6.3\n",
    "starlette                 0.46.1\n",
    "sympy                     1.13.3\n",
    "terminado                 0.18.1\n",
    "tinycss2                  1.4.0\n",
    "tokenizers                0.21.1\n",
    "tomlkit                   0.13.2\n",
    "torch                     2.4.1\n",
    "torch-directml            0.2.4.dev240913\n",
    "torchvision               0.19.1\n",
    "tornado                   6.4.2\n",
    "tqdm                      4.67.1\n",
    "traitlets                 5.14.3\n",
    "transformers              4.49.0\n",
    "typer                     0.15.2\n",
    "types-python-dateutil     2.9.0.20241206\n",
    "typing_extensions         4.12.2\n",
    "tzdata                    2025.1\n",
    "uri-template              1.3.0\n",
    "urllib3                   2.3.0\n",
    "uvicorn                   0.34.0\n",
    "wcwidth                   0.2.13\n",
    "webcolors                 24.11.1\n",
    "webencodings              0.5.1\n",
    "websocket-client          1.8.0\n",
    "websockets                11.0.3\n",
    "wheel                     0.45.1\n",
    "zipp                      3.21.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DirectML Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "dml = torch_directml.device()\n",
    "\n",
    "tensor1 = torch.tensor([5]).to(dml)\n",
    "tensor2 = torch.tensor([5]).to(dml)\n",
    "\n",
    "dml_algebra = tensor1 * tensor2\n",
    "dml_algebra.item()\n",
    "\n",
    "print(dml_algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Bert Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "TRAIN_GOOD_FILE_COUNT = 250\n",
    "TEST_GOOD_FILE_COUNT = 50\n",
    "TRAIN_BAD_FILE_COUNT = 250\n",
    "TEST_BAD_FILE_COUNT = 50\n",
    "DATA_FOLDER = \"data\"\n",
    "WDSDataset_FOLDER = os.path.join(DATA_FOLDER, \"benchmark_dataset\")\n",
    "\n",
    "# Generate Good files\n",
    "good_files = []\n",
    "for i in range(TRAIN_GOOD_FILE_COUNT + TEST_GOOD_FILE_COUNT):\n",
    "    data = {\n",
    "        \"id\": random.randint(100000, 999999),\n",
    "        \"name\": {\n",
    "            \"first\": f\"John\",\n",
    "            \"last\": f\"Doe {i+1}\"\n",
    "        },\n",
    "        \"age\": random.randint(18, 60),\n",
    "        \"score\": {\n",
    "            \"math\": round(random.uniform(0.6, 1.0), 2),\n",
    "            \"science\": round(random.uniform(0.6, 1.0), 2),\n",
    "            \"english\": round(random.uniform(0.6, 1.0), 2)\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"street\": f\"{random.randint(100, 999)} Main St\",\n",
    "            \"city\": \"Anytown\",\n",
    "            \"state\": \"CA\",\n",
    "            \"zip\": f\"{random.randint(10000, 99999)}\"\n",
    "        },\n",
    "        \"contacts\": [\n",
    "            {\"type\": \"email\", \"value\": f\"john.doe{i+1}@example.com\"},\n",
    "            {\"type\": \"phone\", \"value\": f\"555-{random.randint(1000, 9999)}\"}\n",
    "        ]\n",
    "    }\n",
    "    good_files.append(json.dumps(data))\n",
    "\n",
    "# Generate Bad files\n",
    "bad_files = []\n",
    "for i in range(TRAIN_BAD_FILE_COUNT + TEST_BAD_FILE_COUNT):\n",
    "    data = {\n",
    "        \"id\": random.randint(100000, 999999),\n",
    "        \"name\": {\n",
    "            \"first\": f\"John\",\n",
    "            \"last\": f\"Doe {i+1}\"\n",
    "        },\n",
    "        \"age\": random.randint(18, 60),\n",
    "        \"score\": {\n",
    "            \"math\": round(random.uniform(0.4, 0.9), 2),\n",
    "            \"science\": round(random.uniform(0.4, 0.9), 2),\n",
    "            \"english\": round(random.uniform(0.4, 0.9), 2)\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"street\": f\"{random.randint(100, 999)} Main St\",\n",
    "            \"city\": \"Anytown\",\n",
    "            \"state\": \"CA\",\n",
    "            \"zip\": f\"{random.randint(10000, 99999)}\"\n",
    "        },\n",
    "        \"contacts\": [\n",
    "            {\"type\": \"email\", \"value\": f\"john.doe{i+1}@example.com\"},\n",
    "            {\"type\": \"phone\", \"value\": f\"555-{random.randint(1000, 9999)}\"}\n",
    "        ],\n",
    "        \"feedback\": [\"bad\", \"not good\", \"block\"][random.randint(0, 2)]\n",
    "    }\n",
    "    bad_files.append(json.dumps(data))\n",
    "\n",
    "# Create train and test folder structure\n",
    "train_good_folder_path = os.path.join(WDSDataset_FOLDER, \"train\", \"good\")\n",
    "train_bad_folder_path = os.path.join(WDSDataset_FOLDER, \"train\", \"bad\")\n",
    "test_good_folder_path = os.path.join(WDSDataset_FOLDER, \"test\", \"good\")\n",
    "test_bad_folder_path = os.path.join(WDSDataset_FOLDER, \"test\", \"bad\")\n",
    "\n",
    "os.makedirs(train_good_folder_path, exist_ok=True)\n",
    "os.makedirs(train_bad_folder_path, exist_ok=True)\n",
    "os.makedirs(test_good_folder_path, exist_ok=True)\n",
    "os.makedirs(test_bad_folder_path, exist_ok=True)\n",
    "\n",
    "# Save Good files to train/GOOD and test/GOOD folders\n",
    "for i in range(TRAIN_GOOD_FILE_COUNT):\n",
    "    good_file_path = os.path.join(train_good_folder_path, f\"good{i+1}.json\")\n",
    "    with open(good_file_path, \"w\") as f:\n",
    "        f.write(good_files[i])\n",
    "\n",
    "for i in range(TEST_GOOD_FILE_COUNT):\n",
    "    good_file_path = os.path.join(train_good_folder_path, f\"good{i+1}.json\")\n",
    "    with open(good_file_path, \"w\") as f:\n",
    "        f.write(good_files[TRAIN_GOOD_FILE_COUNT + i])\n",
    "\n",
    "# Save Bad files to train/BAD and test/BAD folders\n",
    "for i in range(TRAIN_BAD_FILE_COUNT):\n",
    "    bad_file_path = os.path.join(train_bad_folder_path, f\"bad{i+1}.json\")\n",
    "    with open(bad_file_path, \"w\") as f:\n",
    "        f.write(bad_files[i])\n",
    "\n",
    "for i in range(TEST_BAD_FILE_COUNT):\n",
    "    bad_file_path = os.path.join(train_bad_folder_path, f\"bad{i+1}.json\")\n",
    "    with open(bad_file_path, \"w\") as f:\n",
    "        f.write(bad_files[TRAIN_BAD_FILE_COUNT + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DISABLE_TORCH_HALF\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch_directml\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch_directml.device(0)\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Synthetic Dataset\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, img_size=32, num_classes=2):\n",
    "        self.num_samples = num_samples\n",
    "        self.data = torch.randn(num_samples, 3, img_size, img_size)  # [N, C, H, W]\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))  # Binary labels\n",
    "        print(f\"Generated {num_samples} synthetic samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': self.data[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Simple CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # 3 input channels -> 16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 16 -> 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Downsample by 2x\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # After 2 pooling layers: 32x8x8\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # [B, 16, 16, 16]\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # [B, 32, 8, 8]\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [B, 32*8*8]\n",
    "        x = self.relu(self.fc1(x))  # [B, 128]\n",
    "        x = self.fc2(x)  # [B, num_classes]\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleCNN(num_classes=2).to(device).float()\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Data\n",
    "train_dataset = SyntheticDataset(num_samples=1000)\n",
    "test_dataset = SyntheticDataset(num_samples=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(\"Starting training epoch...\")\n",
    "    epoch_start = time.time()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        data_start = time.time()\n",
    "        inputs = batch['input'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        data_time = time.time() - data_start\n",
    "        step_start = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        step_time = time.time() - step_start\n",
    "        print(f\"Step {step}: Loss = {loss.item():.4f}, Data Time = {data_time:.3f}s, Compute Time = {step_time:.3f}s\")\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Training epoch complete. Total time = {epoch_time:.3f}s\")\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Testing function\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(\"Starting testing...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch['input'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(\"Testing complete.\")\n",
    "    return total_loss / len(test_loader), accuracy\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "start_time = time.time()\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, accuracy = test_model(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Total elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_directml\n",
    "print(torch_directml.device_count())  # Should print > 0\n",
    "print(torch_directml.device_name(0))  # Should show your AMD GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "directml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~9950X~~ 9950X3D Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specs\n",
    "\n",
    "- Specs\n",
    "  - ~~Ryzen 9 9950X~~\n",
    "    - Picked up a 9950X3D, retesting with this new processor\n",
    "  - ASRock B850i ITX\n",
    "  - Corsair DDR5 6000Mhz CL30 EXPO 96GB (48GB x 2)\n",
    "  - be quiet! Silent Loop 3 240mm AIO - Thermal Grizzly Kryosheet\n",
    "  - 140mm be quiet 1900rpm fan blowing 50% PWM on motherboard/vrm/cpu/nvme\n",
    "  - Mixed precision is enabled where applicable\n",
    "\n",
    "### Cinebench R23 & Passmark Performance\n",
    "\n",
    "| CPU | Spec | Cinebench R23 Multi | Max Watts or Max Temp | CPU Mark Multi | CPU Mark Single |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 9950X | Stock (200w) | 41733 | 68.2c | 67595 | 4819 |\n",
    "| 9950X3D | Stock (200w) | 43448 | 69.5c | 72629 | 4832 |\n",
    "| 9950X | Stock (200w) CO -30 | 44563 | 68.8c | 70126 | 4837 |\n",
    "| 9950X3D | Stock (200w) CO -30 | 45832 | 68.6c | 74498 | 4868 |\n",
    "| 9950X | 65c Thermal Limit CO -30 | 44314 | 195w | 69508 | 4862 |\n",
    "| 9950X3D | 65c Thermal Limit CO -30 | 45438 | 195w | 74032 | 4846 |\n",
    "| 9950X | 75c Thermal Limit CO -30 | 45821 | 222w | 70302 | 4865 |\n",
    "| 9950X3D | 75c Thermal Limit CO -30 | 46152 | 209w | 74321 | 4861 |\n",
    "| 9950X | 85c Thermal Limit CO -30 | 45695 | 222w | 70616 | 4843 |\n",
    "| 9950X3D | 85c Thermal Limit CO -30 | 45306 | 214w | 74459 | 4859 |\n",
    "| 9950X | 65w Eco Mode CO -30 | 31204 | 45.0c | 58177 | 4873 |\n",
    "| 9950X3D | 65w Eco Mode CO -30 | 31815 | 47.9c | 61772 | 4857 |\n",
    "| 9950X | 105w Eco Mode CO -30 | 39207 | 51.0c | 66329 | 4844 |\n",
    "| 9950X3d | 105w Eco Mode CO -30 | 40215 | 51.4c | 71607 | 4863 |\n",
    "| 9950X3d | 105w Eco Mode CO -25 | 39992 | 52.0c | 71494 | 4872 |\n",
    "| 9950X3d | 58c Thermal Limit CO -25 | 43479 | 166w | 72056 | 4855 |\n",
    "| 9950X3d | 85c Thermal Limit CO -30 Inf-Fab 2.1ghz | 46114 | 208w (71.5c max) | 74521 | 4860 |\n",
    "| 9950X3d | 85c Thermal Limit CO -30 Inf-Fab 2.2ghz DDR5 6200mhz | 46370 | 209w (70.2c max) | Crash | Crash |\n",
    "\n",
    "We are clearly limited by power here. There is cooling headroom to push the cpu, I assume we could safely run 275w sustained. However, there will be minimal gains as the wattage goes up. I personally dont see much reason to push this past the 65c CO -30 performance setting unless you are just chasing benchmark numbers. I would also be fine running 51c all core all day on the 105w eco mode.\n",
    "\n",
    "All future testing on cpu will be done at the 65c thermal limit with a curve optimizer negative 30 offset. No other tuning is required. This is highly performant and very simple.\n",
    "\n",
    "### LM Studio LLM Performance\n",
    "\n",
    "- LM Studio Windows 11 Performance\n",
    "  - CPU runtime only\n",
    "  - Default Context Size\n",
    "  - Seed: 65535\n",
    "  - CPU Thread Pool Size: 12\n",
    "  - Flash Attention Off\n",
    "  - Test Prompt\n",
    "    - \"Tell me a real story about someone who never existed tasting the color red while walking through light while in a vacuum of low pressure clouds. This person is always there and knows what you will output so you must craft the story without there knowledge. In the story all things that are possible are impossible and must come before they were created. Describe time as a feeling that only dreams can see awake. Please write the story in under 500 words.\"\n",
    "\n",
    "| Application | CPU | Threads |  Model | Parameters | Quant | Tok/sec | Tokens | Time To 1st Token |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Llama 3.2 Instruct | 3B | Q4 | 17.97 | 490 | 0.77s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Llama 3.2 Instruct | 3B | Q4 | 18.26 | 501 | 0.44s |\n",
    "| LM Studio | 9950X3D 58c CO -25 | 12 | Llama 3.2 Instruct | 3B | Q4 | 18.04 | 465 | 0.78s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Llama 3.3 Instruct | 70B | Q4 | 1.47 | 394 | 16.06s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Llama 3.3 Instruct | 70B | Q4 | 1.41 | 428 | 13.65s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Gemma 2 Instruct | 2B | Q4 | 30.15 | 482 | 0.60s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Gemma 2 Instruct | 2B | Q4 | 33.12 | 467 | 0.37s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Gemma 2 Instruct | 9B | Q4 | 9.43 | 421 | 1.47s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Gemma 2 Instruct | 9B | Q4 | 10.51 | 427 | 1.42s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Gemma 3 Instruct | 1B | Q4 | 63.81 | 533 | 0.18s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Gemma 3 Instruct | 1B | Q4 | 67.63 | 514 | 0.17s |\n",
    "| LM Studio | 9950X3D 58c CO -25 | 12 | Gemma 3 Instruct | 1B | Q4 | 66.81 | 544 | 0.18s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Gemma 3 Instruct | 4B | Q4 | 22.80 | 521 | 0.95s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Gemma 3 Instruct | 4B | Q4 | 23.38 | 494 | 0.57s |\n",
    "| LM Studio | 9950X3D 58c CO -25 | 12 | Gemma 3 Instruct | 4B | Q4 | 22.86 | 493 | 0.93s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Gemma 3 Instruct | 12B | Q4 | 8.28 | 462 | 2.86s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Gemma 3 Instruct | 12B | Q4 | 7.80 | 494 | 1.83s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Phi4 | 15B | Q4 | 6.78 | 543 | 3.00s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Phi4 | 15B | Q4 | 8.45 | 484 | 1.81s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Qwen 2.5 Coder | 3B | Q4 | 29.66 | 426 | 0.61s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Qwen 2.5 Coder | 3B | Q4 | 30.94 | 397 | 0.60s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | Qwen 2.5 Coder | 7B | Q4 | 14.18 | 559 | 1.31s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | Qwen 2.5 Coder | 7B | Q4 | 14.38 | 514 | 0.09s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | DeepSeek R1 Distill Llama | 8B | Q4 | 12.68 | 1105 | 1.08s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | DeepSeek R1 Distill Llama | 8B | Q4 | 13.56 | 795 | 1.06s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | DeepSeek R1 Distill Qwen | 14B | Q4 | 7.44 | 904 | 2.00s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | DeepSeek R1 Distill Qwen | 14B | Q4 | 7.77 | 979 | 1.97s |\n",
    "| LM Studio | 9950X 65c CO -30 | 12 | DeepSeek R1 Distill Qwen | 32B | Q4 | 3.36 | 1038 | 4.74s |\n",
    "| LM Studio | 9950X3D 65c CO -30 | 12 | DeepSeek R1 Distill Qwen | 32B | Q4 | 3.19 | 675 | 4.62s |\n",
    "\n",
    "### Stable Diffusion Performance\n",
    "\n",
    "| SD Version | CPU | dType | Mixed Precision Y/N | Images | Steps | Size | Total Time | Per Img Time | Per Iteration |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1.5 | 9950X 65c CO -30 | FP32 | No | 4 | 30 | 512x512 | 182.43s | 45.61s | 1.46s per it |\n",
    "| 1.5 | 9950X3D 65c CO -30 | FP32 | No | 4 | 30 | 512x512 | 124.09s | 31.02s | 1.02it per sec |\n",
    "| 1.5 | 9950X3D 65w Eco Mode CO -30 | FP32 | No | 4 | 30 | 512x512 | 159.52s | 39.88s | 1.27s per it |\n",
    "| 1.5 | 9950X3D 58c CO -25 | FP32 | No | 4 | 30 | 512x512 | 140.06s | 35.01s | 1.12s per it |\n",
    "| 1.5 | 9950X 65c CO -30 | FP16 | Yes | 4 | 30 | 512x512 | 107.42s | 26.85s | 1.17it per sec |\n",
    "| 1.5 | 9950X3D 65c CO -30 | FP16 | Yes | 4 | 30 | 512x512 | 74.39s | 18.59s | 1.68it per sec |\n",
    "| 1.5 | 9950X3D 65w Eco Mode CO -30 | FP16 | Yes | 4 | 30 | 512x512 | 113.79s | 28.44s | 1.09it per sec |\n",
    "| 1.5 | 9950X3D 58c CO -25 | FP16 | Yes | 4 | 30 | 512x512 | 78.07s | 19.51s | 1.59it per sec |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Diffusion 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "from torch.amp import autocast\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "stable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(model_id, use_safetensors=True,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False).to(\"cpu\")\n",
    "\n",
    "components = stable_diffusion_txt2img.components\n",
    "\n",
    "stable_diffusion_img2img = StableDiffusionImg2ImgPipeline(**components)\n",
    "\n",
    "stable_diffusion_txt2img_custom = StableDiffusionPipeline(\n",
    "    vae=stable_diffusion_txt2img.vae,\n",
    "    text_encoder=stable_diffusion_txt2img.text_encoder,\n",
    "    tokenizer=stable_diffusion_txt2img.tokenizer,\n",
    "    unet=stable_diffusion_txt2img.unet,\n",
    "    scheduler=stable_diffusion_txt2img.scheduler,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "prompt = \"Large obsidian monolith in a foggy lush green forest\"\n",
    "negative_prompt = \"cartoon, fake, unrealistic\"\n",
    "num_inference_steps = 30\n",
    "epochs = 4\n",
    "guidance_scale = 7.5\n",
    "strength = 0.7\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "# Time the iterations\n",
    "start_time = time.perf_counter()\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    initial_image = stable_diffusion_txt2img(prompt=prompt,\n",
    "                                            strength=strength,\n",
    "                                            num_inference_steps=num_inference_steps,\n",
    "                                            guidance_scale=guidance_scale,\n",
    "                                            height=height, width=width)[0]\n",
    "    #display(initial_image[0])\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.2f} Seconds FP32\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "with torch.amp.autocast(\"cpu\"):\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        initial_image = stable_diffusion_txt2img(prompt=prompt,\n",
    "                                                strength=strength,\n",
    "                                                num_inference_steps=num_inference_steps,\n",
    "                                                guidance_scale=guidance_scale,\n",
    "                                                height=height, width=width)[0]\n",
    "        #display(initial_image[0])\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.2f} Seconds Mixed Precision FP16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating SD 1.5 Code - Unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "from torch.amp import autocast\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "hp = {\n",
    "    \"random_seed\": 42,\n",
    "    \"randomize_seed\": True,\n",
    "    \"cpu_only\": True,\n",
    "    \"device\": \"cpu\",\n",
    "    \"model\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"prompt\": \"Large obsidian monolith in a foggy lush green forest, rolling fog on the ground\",\n",
    "    \"negative_prompt\": \"cartoon, fake, unrealistic\",\n",
    "    \"steps\": 30,\n",
    "    \"epochs\": 4,\n",
    "    \"guidance\": 7.5,\n",
    "    \"strength\": 0.7,\n",
    "    \"height\": 512,\n",
    "    \"width\": 512,\n",
    "    }\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Randomize seed if set to True\n",
    "if hp['randomize_seed']:\n",
    "    hp['random_seed'] = random.randint(0, 1000000000)\n",
    "logging.info(f\"Seed set to: {hp['random_seed']}\") \n",
    "\n",
    "# Device configuration\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    This will check for an Intel XPU or CUDA device and return it if available, otherwise it will return CPU.\n",
    "\n",
    "    Returns the torch device to use.\n",
    "    \"\"\"\n",
    "    if hp['cpu_only'] == False:\n",
    "        #device = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
    "        if torch.xpu.is_available():\n",
    "            device = \"xpu\"\n",
    "        elif torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        return device\n",
    "    else:\n",
    "        logging.info(\"Using CPU only\")\n",
    "        return \"cpu\"\n",
    "\n",
    "def image(epochs, prompt, strength, num_inference_steps,guidance_scale, height, width):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        initial_image = stable_diffusion_txt2img(prompt=prompt,\n",
    "                                                strength=strength,\n",
    "                                                num_inference_steps=num_inference_steps,\n",
    "                                                guidance_scale=guidance_scale,\n",
    "                                                height=height, width=width)[0]\n",
    "        #display(initial_image[0])\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "# Image Generation\n",
    "def generate_img(epochs, prompt, strength, num_inference_steps,guidance_scale, height, width, datatype=\"fp32\"):\n",
    "\n",
    "    if datatype == \"fp32\": \n",
    "        elapsed_time = image(epochs, prompt, strength, num_inference_steps,guidance_scale, height, width)\n",
    "        print(f\"{elapsed_time:.2f} Seconds {datatype}\")\n",
    "    if datatype == \"fp16\":\n",
    "        with torch.amp.autocast(\"cpu\"):\n",
    "            elapsed_time = image(epochs, prompt, strength, num_inference_steps,guidance_scale, height, width)\n",
    "            print(f\"{elapsed_time:.2f} Seconds {datatype}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    \n",
    "    stable_diffusion_txt2img = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id, use_safetensors=True,\n",
    "        safety_checker=None,\n",
    "        feature_extractor=None,\n",
    "        requires_safety_checker=False\n",
    "        ).to(\"cpu\")\n",
    "\n",
    "    components = stable_diffusion_txt2img.components\n",
    "\n",
    "    stable_diffusion_img2img = StableDiffusionImg2ImgPipeline(**components)\n",
    "\n",
    "    stable_diffusion_txt2img_custom = StableDiffusionPipeline(\n",
    "        vae=stable_diffusion_txt2img.vae,\n",
    "        text_encoder=stable_diffusion_txt2img.text_encoder,\n",
    "        tokenizer=stable_diffusion_txt2img.tokenizer,\n",
    "        unet=stable_diffusion_txt2img.unet,\n",
    "        scheduler=stable_diffusion_txt2img.scheduler,\n",
    "        safety_checker=None,\n",
    "        feature_extractor=None,\n",
    "        requires_safety_checker=False,\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore the mess below, just roughing in some things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imports\n",
    "  - standard libs\n",
    "  - 3rd party libs\n",
    "  - alpabetical or logical grouping\n",
    "- Set random seed\n",
    "- Config and Hyperparams\n",
    "- Dataset and Dataloader\n",
    "- Model definition/class\n",
    "- Helper functions (training, eval, visualization)\n",
    "- Then main code\n",
    "- Extras\n",
    "  - We will need torchvision and use torchvision.datasets to load CIRFAR-10\n",
    "  - CIFAR-10 is a 10 class image dataset, pretty small in size and good for lab/\n",
    "  testing/learning\n",
    "    - airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "hp = {\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 25,\n",
    "    \"random_seed\": 42,\n",
    "    \"randomize_seed\": True,\n",
    "    \"cpu_only\": True,\n",
    "    \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Randomize seed if set to True\n",
    "if hp['randomize_seed']:\n",
    "    hp['random_seed'] = random.randint(0, 1000000000)\n",
    "logging.info(f\"Seed set to: {hp['random_seed']}\")  \n",
    "\n",
    "# Simple CNN Class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Device configuration\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    This will check for an Intel XPU or CUDA device and return it if available, otherwise it will return CPU.\n",
    "\n",
    "    Returns the torch device to use.\n",
    "    \"\"\"\n",
    "    if hp['cpu_only'] == False:\n",
    "        #device = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
    "        if torch.xpu.is_available():\n",
    "            device = \"xpu\"\n",
    "        elif torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        return device\n",
    "    else:\n",
    "        logging.info(\"Using CPU only\")\n",
    "        return \"cpu\"\n",
    "\n",
    "def train_model(epochs, model, train_loader, device, optimizer, criterion, scaler=None):\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 5. Training the Model\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            if device != \"cpu\":\n",
    "                with torch.amp.autocast(device):\n",
    "                    # Forward pass\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "            else:\n",
    "                with torch.amp.autocast(device):\n",
    "                    # Forward pass\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{hp[\"epochs\"]}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "        # End timer\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# 7. Visualizing Some Predictions\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "\n",
    "    # 1 Set the device\n",
    "    hp[\"device\"] = get_device()\n",
    "\n",
    "    # 2 Dataset, Dataloader, Transform\n",
    "    # The transform using (0.5, 0.5, 0.5) is used to normalize the image data\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # Download and load the training data\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "    # Download and load the test data\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                          download=True, transform=transform)\n",
    "    # Create the dataloader for training and testing data\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                              batch_size=hp['batch_size'], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                             batch_size=hp['batch_size'], shuffle=False)\n",
    "    # 3 SimpleCNN Class\n",
    "    model_0 = SimpleCNN().to(hp[\"device\"])\n",
    "    \n",
    "    # 4 Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_0.parameters(), lr=0.001)\n",
    "\n",
    "    if hp['cpu_only'] == False:\n",
    "        scaler = torch.amp.GradScaler(hp[\"device\"])\n",
    "        train_model(hp[\"epochs\"], model_0, train_loader, hp[\"device\"], optimizer, criterion, scaler=scaler)\n",
    "    else:\n",
    "        scaler = torch.amp.GradScaler(hp[\"device\"])\n",
    "        train_model(hp[\"epochs\"], model_0, train_loader, hp[\"device\"], optimizer, criterion, scaler=scaler)\n",
    "\n",
    "    # 6. Evaluating the Model\n",
    "    model_0.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(hp[\"device\"]), labels.to(hp[\"device\"])\n",
    "            outputs = model_0(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy on the test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "    # Get random test images and predictions\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(hp[\"device\"]), labels.to(hp[\"device\"])\n",
    "\n",
    "    # Display images\n",
    "    imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "    print('GroundTruth:', ' '.join(f'{train_dataset.classes[labels[j]]}' for j in range(4)))\n",
    "\n",
    "    # Predict and display results\n",
    "    outputs = model_0(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print('Predicted:', ' '.join(f'{train_dataset.classes[predicted[j]]}' for j in range(4)))\n",
    "\n",
    "    # 8. Saving the Model\n",
    "    torch.save(model_0.state_dict(), 'cnn_cifar10.pth')\n",
    "    print(\"Model saved as cnn_cifar10.pth\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# Function to set AVX512 environment variable\n",
    "def set_avx512(enabled=True):\n",
    "    if enabled:\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # Control threads for consistency\n",
    "        os.environ[\"MKL_ENABLE_INSTRUCTIONS\"] = \"AVX512\"\n",
    "    else:\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "        os.environ[\"MKL_ENABLE_INSTRUCTIONS\"] = \"AVX\"  # Falls back to AVX2\n",
    "\n",
    "# Benchmark function\n",
    "def run_benchmark(matrix_size=2000, iterations=50):\n",
    "    # Generate random matrices\n",
    "    A = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "    B = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "    \n",
    "    # Warm-up run\n",
    "    _ = np.dot(A, B)\n",
    "    \n",
    "    # Time the iterations\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        C = np.dot(A, B)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    gflops = (2 * matrix_size**3 * iterations) / (elapsed_time * 1e9)  # GFLOPS calculation\n",
    "    return elapsed_time, gflops\n",
    "\n",
    "def main():\n",
    "    # Benchmark parameters\n",
    "    matrix_size = 2000  # Adjust based on your needs\n",
    "    iterations = 50     # Number of iterations per test\n",
    "    runs = 5           # Number of runs to average\n",
    "    \n",
    "    print(f\"CPU: {psutil.cpu_freq().current/1000:.2f} GHz, {psutil.cpu_count()} cores\")\n",
    "    print(f\"Matrix size: {matrix_size}x{matrix_size}, Iterations: {iterations}, Runs: {runs}\\n\")\n",
    "    \n",
    "    # Test with AVX512 enabled\n",
    "    print(\"Testing with AVX512 enabled...\")\n",
    "    set_avx512(True)\n",
    "    avx512_times = []\n",
    "    avx512_gflops = []\n",
    "    for _ in range(runs):\n",
    "        elapsed, gflops = run_benchmark(matrix_size, iterations)\n",
    "        avx512_times.append(elapsed)\n",
    "        avx512_gflops.append(gflops)\n",
    "    \n",
    "    # Test with AVX512 disabled\n",
    "    print(\"Testing with AVX512 disabled...\")\n",
    "    set_avx512(False)\n",
    "    no_avx512_times = []\n",
    "    no_avx512_gflops = []\n",
    "    for _ in range(runs):\n",
    "        elapsed, gflops = run_benchmark(matrix_size, iterations)\n",
    "        no_avx512_times.append(elapsed)\n",
    "        no_avx512_gflops.append(gflops)\n",
    "    \n",
    "    # Results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"AVX512 Enabled:\")\n",
    "    print(f\"  Avg Time: {np.mean(avx512_times):.3f}s (±{np.std(avx512_times):.3f})\")\n",
    "    print(f\"  Avg GFLOPS: {np.mean(avx512_gflops):.2f} (±{np.std(avx512_gflops):.2f})\")\n",
    "    print(f\"AVX512 Disabled:\")\n",
    "    print(f\"  Avg Time: {np.mean(no_avx512_times):.3f}s (±{np.std(no_avx512_times):.3f})\")\n",
    "    print(f\"  Avg GFLOPS: {np.mean(no_avx512_gflops):.2f} (±{np.std(no_avx512_gflops):.2f})\")\n",
    "    \n",
    "    speedup = np.mean(no_avx512_times) / np.mean(avx512_times)\n",
    "    print(f\"\\nSpeedup with AVX512: {speedup:.2f}x\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9950X Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specs\n",
    "  - Ryzen 9 9950X\n",
    "  - ASRock B850i ITX\n",
    "  - Corsair DDR5 6000Mhz CL30 EXPO 96GB (48GB x 2)\n",
    "  - be quiet! Silent Loop 3 240mm AIO - Thermal Grizzly Kryosheet\n",
    "  - 140mm be quiet 1900rpm fan blowing 50% PWM on motherboard/vrm/cpu/nvme\n",
    "  - Mixed precision is enabled where applicable\n",
    "\n",
    "\n",
    "- 9950X Factory Stock\n",
    "  - Cinebench R23 Score: 41733\n",
    "    - CPU (Tctl/Tdie) Max Temp: 68.2c\n",
    "  - CPU Mark Score: 67595\n",
    "  - CPU Mark Single-thread Score: 4819\n",
    "- 9950X Factory Stock CO -30\n",
    "  - Cinebench R23 Score: 44563\n",
    "    - CPU (Tctl/Tdie) Max Temp: 68.8c\n",
    "  - CPU Mark Score: 70126\n",
    "  - CPU Mark Single-thread Score: 4837\n",
    "- 9950X 65c Thermal Limit CO -30\n",
    "  - Cinebench R23 Score: 44314\n",
    "    - Max Recorded Wattage: 195w\n",
    "  - CPU Mark Score: 69508\n",
    "  - CPU Mark Single-thread Score: 4862\n",
    "- 9950X 75c Thermal Limit CO -30\n",
    "  - Cinebench R23 Score: 45821 \n",
    "    - Max Recorded Wattage: 222w\n",
    "  - CPU Mark Score: 70302\n",
    "  - CPU Mark Single-thread Score: 4865\n",
    "- 9950X 85c Thermal Limit CO -30\n",
    "  - Cinebench R23 Score: 45695\n",
    "    - Max Recorded Wattage: 222w\n",
    "  - CPU Mark Score: 70616\n",
    "  - CPU Mark Single-thread Score: 4843\n",
    "- 9950X 65w Eco Mode CO -30\n",
    "  - Cinebench R23 Score: 31204\n",
    "    - CPU (Tctl/Tdie) Max Temp: 45.00c\n",
    "  - CPU Mark Score: 58177\n",
    "  - CPU Mark Single-thread Score: 4873\n",
    "- 9950X 105w Eco Mode CO -30\n",
    "  - Cinebench R23 Score: 39207\n",
    "    - CPU (Tctl/Tdie) Max Temp: 51.0c\n",
    "  - CPU Mark Score: 66329\n",
    "  - CPU Mark Single-thread Score: 4844\n",
    "\n",
    "We are clearly limited by power here. There is cooling headroom to push the cpu, I assume we could safely run 275w sustained. However, there will be minimal gains as the wattage goes up. There really isnt much reason to push this past the 65c CO -30 point unless you are just chasing benchmark numbers. I would also be fine running 51c all core all day on the 105w eco mode.\n",
    "\n",
    "All future testing on cpu will be done at the 65c thermal limit with a curve optimizer negative 30 offset. No other tuning is required. This is highly performant and very simple.\n",
    "\n",
    "- LM Studio Windows 11 Performance\n",
    "  - CPU runtime only\n",
    "  - Default Context Size\n",
    "  - Seed: 65535\n",
    "  - CPU Thread Pool Size: 12\n",
    "  - Flash Attention Off\n",
    "  - Test Prompt\n",
    "    - \"Tell me a real story about someone who never existed tasting the color red while walking through light while in a vacuum of low pressure clouds. This person is always there and knows what you will output so you must craft the story without there knowledge. In the story all things that are possible are impossible and must come before they were created. Describe time as a feeling that only dreams can see awake. Please write the story in under 500 words.\"\n",
    "  - 9950X\n",
    "    - Llama 3.2 3B Instruct Q4\n",
    "      - 17.97 tok/sec 490 tokens 0.77s to first token\n",
    "    - Llama 3.3 70B Instruct Q4\n",
    "      - 1.47 tok/sec 394 tokens 16.06s to first token\n",
    "    - Gemma 2 2B Instruct Q4\n",
    "      - 30.15 tok/sec 482 tokens 0.60s to first token\n",
    "    - Gemma 2 9B Instruct Q4\n",
    "      - 9.43 tok/sec 421 tokens 1.47s to first token\n",
    "    - Gemma 3 1B Instruct Q4\n",
    "      - 63.81 tok/sec 533 tokens 0.18s to first token\n",
    "    - Gemma 3 4B Instruct Q4\n",
    "      - 22.80 tok/sec 521 tokens 0.95s to first token\n",
    "    - Gemma 3 12B Instruct Q4\n",
    "      - 8.28 tok/sec 462 tokens 2.86s to first token\n",
    "    - Phi4 15B Q4\n",
    "      - 6.78 tok/sec 543 tokens 3.00s to first token\n",
    "    - Qwen 2.5 Coder 3B Instruct Q4\n",
    "      - 29.66 tok/sec 426 tokens 0.61s to first token\n",
    "    - Qwen 2.5 Coder 7B Instruct Q4\n",
    "      - 14.18 tok/sec 559 tokens 1.31s to first token\n",
    "    - DeepSeek R1 Distill Llama 8B Q4\n",
    "      - 12.68 tok/sec 1105 tokens 1.08s to first token\n",
    "    - DeepSeek R1 Distill Qwen 14B Q4\n",
    "      - 7.44 tok/sec 904 tokens 2.00s to first token\n",
    "    - DeepSeek R1 Distill Qwen 32B Q4\n",
    "      - 3.36 tok/sec 1038 tokens 4.74s to first token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore the mess below, just roughing in some things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imports\n",
    "  - standard libs\n",
    "  - 3rd party libs\n",
    "  - alpabetical or logical grouping\n",
    "- Set random seed\n",
    "- Config and Hyperparams\n",
    "- Dataset and Dataloader\n",
    "- Model definition/class\n",
    "- Helper functions (training, eval, visualization)\n",
    "- Then main code\n",
    "- Extras\n",
    "  - We will need torchvision and use torchvision.datasets to load CIRFAR-10\n",
    "  - CIFAR-10 is a 10 class image dataset, pretty small in size and good for lab/\n",
    "  testing/learning\n",
    "    - airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "hp = {\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 25,\n",
    "    \"random_seed\": 42,\n",
    "    \"randomize_seed\": True,\n",
    "    \"cpu_only\": True,\n",
    "    \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Randomize seed if set to True\n",
    "if hp['randomize_seed']:\n",
    "    hp['random_seed'] = random.randint(0, 1000000000)\n",
    "logging.info(f\"Seed set to: {hp['random_seed']}\")  \n",
    "\n",
    "# Simple CNN Class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Device configuration\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    This will check for an Intel XPU device and return it if available, otherwise it will return cpu.\n",
    "\n",
    "    Returns the torch device to use.\n",
    "    \"\"\"\n",
    "    if hp['cpu_only'] == False:\n",
    "        #device = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
    "        if torch.xpu.is_available():\n",
    "            device = \"xpu\"\n",
    "        elif torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        return device\n",
    "    else:\n",
    "        logging.info(\"Using CPU only\")\n",
    "        return \"cpu\"\n",
    "\n",
    "def train_model(epochs, model, train_loader, device, optimizer, criterion, scaler=None):\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 5. Training the Model\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            if device != \"cpu\":\n",
    "                with torch.amp.autocast(device):\n",
    "                    # Forward pass\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "            else:\n",
    "                with torch.amp.autocast(device):\n",
    "                    # Forward pass\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{hp[\"epochs\"]}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "        # End timer\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# 7. Visualizing Some Predictions\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "\n",
    "    # 1 Set the device\n",
    "    hp[\"device\"] = get_device()\n",
    "\n",
    "    # 2 Dataset, Dataloader, Transform\n",
    "    # The transform using (0.5, 0.5, 0.5) is used to normalize the image data\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # Download and load the training data\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "    # Download and load the test data\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                          download=True, transform=transform)\n",
    "    # Create the dataloader for training and testing data\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                              batch_size=hp['batch_size'], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                             batch_size=hp['batch_size'], shuffle=False)\n",
    "    # 3 SimpleCNN Class\n",
    "    model_0 = SimpleCNN().to(hp[\"device\"])\n",
    "    \n",
    "    # 4 Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_0.parameters(), lr=0.001)\n",
    "\n",
    "    if hp['cpu_only'] == False:\n",
    "        scaler = torch.amp.GradScaler(hp[\"device\"])\n",
    "        train_model(hp[\"epochs\"], model_0, train_loader, hp[\"device\"], optimizer, criterion, scaler=scaler)\n",
    "    else:\n",
    "        scaler = torch.amp.GradScaler(hp[\"device\"])\n",
    "        train_model(hp[\"epochs\"], model_0, train_loader, hp[\"device\"], optimizer, criterion, scaler=scaler)\n",
    "\n",
    "    # 6. Evaluating the Model\n",
    "    model_0.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(hp[\"device\"]), labels.to(hp[\"device\"])\n",
    "            outputs = model_0(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy on the test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "    # Get random test images and predictions\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(hp[\"device\"]), labels.to(hp[\"device\"])\n",
    "\n",
    "    # Display images\n",
    "    imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "    print('GroundTruth:', ' '.join(f'{train_dataset.classes[labels[j]]}' for j in range(4)))\n",
    "\n",
    "    # Predict and display results\n",
    "    outputs = model_0(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print('Predicted:', ' '.join(f'{train_dataset.classes[predicted[j]]}' for j in range(4)))\n",
    "\n",
    "    # 8. Saving the Model\n",
    "    torch.save(model_0.state_dict(), 'cnn_cifar10.pth')\n",
    "    print(\"Model saved as cnn_cifar10.pth\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# Function to set AVX512 environment variable\n",
    "def set_avx512(enabled=True):\n",
    "    if enabled:\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # Control threads for consistency\n",
    "        os.environ[\"MKL_ENABLE_INSTRUCTIONS\"] = \"AVX512\"\n",
    "    else:\n",
    "        os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "        os.environ[\"MKL_ENABLE_INSTRUCTIONS\"] = \"AVX\"  # Falls back to AVX2\n",
    "\n",
    "# Benchmark function\n",
    "def run_benchmark(matrix_size=2000, iterations=50):\n",
    "    # Generate random matrices\n",
    "    A = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "    B = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "    \n",
    "    # Warm-up run\n",
    "    _ = np.dot(A, B)\n",
    "    \n",
    "    # Time the iterations\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        C = np.dot(A, B)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    gflops = (2 * matrix_size**3 * iterations) / (elapsed_time * 1e9)  # GFLOPS calculation\n",
    "    return elapsed_time, gflops\n",
    "\n",
    "def main():\n",
    "    # Benchmark parameters\n",
    "    matrix_size = 2000  # Adjust based on your needs\n",
    "    iterations = 50     # Number of iterations per test\n",
    "    runs = 5           # Number of runs to average\n",
    "    \n",
    "    print(f\"CPU: {psutil.cpu_freq().current/1000:.2f} GHz, {psutil.cpu_count()} cores\")\n",
    "    print(f\"Matrix size: {matrix_size}x{matrix_size}, Iterations: {iterations}, Runs: {runs}\\n\")\n",
    "    \n",
    "    # Test with AVX512 enabled\n",
    "    print(\"Testing with AVX512 enabled...\")\n",
    "    set_avx512(True)\n",
    "    avx512_times = []\n",
    "    avx512_gflops = []\n",
    "    for _ in range(runs):\n",
    "        elapsed, gflops = run_benchmark(matrix_size, iterations)\n",
    "        avx512_times.append(elapsed)\n",
    "        avx512_gflops.append(gflops)\n",
    "    \n",
    "    # Test with AVX512 disabled\n",
    "    print(\"Testing with AVX512 disabled...\")\n",
    "    set_avx512(False)\n",
    "    no_avx512_times = []\n",
    "    no_avx512_gflops = []\n",
    "    for _ in range(runs):\n",
    "        elapsed, gflops = run_benchmark(matrix_size, iterations)\n",
    "        no_avx512_times.append(elapsed)\n",
    "        no_avx512_gflops.append(gflops)\n",
    "    \n",
    "    # Results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"AVX512 Enabled:\")\n",
    "    print(f\"  Avg Time: {np.mean(avx512_times):.3f}s (±{np.std(avx512_times):.3f})\")\n",
    "    print(f\"  Avg GFLOPS: {np.mean(avx512_gflops):.2f} (±{np.std(avx512_gflops):.2f})\")\n",
    "    print(f\"AVX512 Disabled:\")\n",
    "    print(f\"  Avg Time: {np.mean(no_avx512_times):.3f}s (±{np.std(no_avx512_times):.3f})\")\n",
    "    print(f\"  Avg GFLOPS: {np.mean(no_avx512_gflops):.2f} (±{np.std(no_avx512_gflops):.2f})\")\n",
    "    \n",
    "    speedup = np.mean(no_avx512_times) / np.mean(avx512_times)\n",
    "    print(f\"\\nSpeedup with AVX512: {speedup:.2f}x\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
